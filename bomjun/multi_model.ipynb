{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450a148-9745-4e51-9e4d-ad7ad4ee2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import copy\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import albumentations\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a541ca-e0e3-450a-b9b2-fd19a8edb076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "SEED = 2021\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)  # type: ignore\n",
    "torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9fa8f-fa71-414b-9f7c-15ac55f57563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 OS 및 라이브러리 버전 체크 체크\n",
    "#current_os = platform.system()\n",
    "#print(f\"Current OS: {current_os}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "#print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"torch Version: {torch.__version__}\")\n",
    "#print(f\"torchvision Version: {torchvision.__version__}\")\n",
    "\n",
    "# 중요하지 않은 에러 무시\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# 유니코드 깨짐현상 해결\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b92cc8-6631-474a-a282-7f66150afa15",
   "metadata": {},
   "source": [
    "# face crop image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e15c3-fc26-4b2b-9621-0fc790f056c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from facenet_pytorch import MTCNN\n",
    "import cv2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "new_img_dir = '../input/data/train/new_imgs/'\n",
    "train_feat_dir = '../input/data/train'\n",
    "test_feat_dir = '../input/data/eval'\n",
    "train_img_dir = '../input/data/train/images/'\n",
    "test_img_dir = '../input/data/eval/images/'\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for paths in os.listdir(train_img_dir):\n",
    "    if paths[0] == '.': continue\n",
    "    os.mkdir(new_img_dir + paths)\n",
    "    sub_dir = os.path.join(train_img_dir, paths)\n",
    "    \n",
    "    for imgs in os.listdir(sub_dir):\n",
    "        if imgs[0] == '.': continue\n",
    "        \n",
    "        img_dir = os.path.join(sub_dir, imgs)\n",
    "        img = cv2.imread(img_dir)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #mtcnn 적용\n",
    "        boxes,probs = mtcnn.detect(img)\n",
    "        \n",
    "        # boxes 확인\n",
    "        if len(probs) > 1:\n",
    "            pass \n",
    "            #print(boxes)\n",
    "        if not isinstance(boxes, np.ndarray):\n",
    "            #print('Nope!')\n",
    "            # 직접 crop\n",
    "            img = img[100:400, 50:350, :]\n",
    "        else: # boexes size 확인\n",
    "            xmin = int(boxes[0, 0])-30\n",
    "            ymin = int(boxes[0, 1])-30\n",
    "            xmax = int(boxes[0, 2])+30\n",
    "            ymax = int(boxes[0, 3])+30\n",
    "            \n",
    "            if xmin < 0: xmin = 0\n",
    "            if ymin < 0: ymin = 0\n",
    "            if xmax > 384: xmax = 384\n",
    "            if ymax > 512: ymax = 512\n",
    "            \n",
    "            img = img[ymin:ymax, xmin:xmax, :]\n",
    "            \n",
    "        tmp = os.path.join(new_img_dir, paths)\n",
    "        cnt += 1\n",
    "        imageio.imwrite(os.path.join(tmp, imgs), img)\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c81d32-e098-48a3-83c3-98a0f3a6d2e1",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f82b8-adad-438d-a09a-c7efcbc4a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "IMG_EXTENSIONS = [\n",
    "    \".jpg\", \".JPG\", \".jpeg\", \".JPEG\", \".png\",\n",
    "    \".PNG\", \".ppm\", \".PPM\", \".bmp\", \".BMP\",\n",
    "]\n",
    "\n",
    "class MaskLabels(int, Enum):\n",
    "    MASK = 0\n",
    "    INCORRECT = 1\n",
    "    NORMAL = 2\n",
    "\n",
    "\n",
    "class GenderLabels(int, Enum):\n",
    "    MALE = 0\n",
    "    FEMALE = 1\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls, value: str) -> int:\n",
    "        value = value.lower()\n",
    "        if value == \"male\":\n",
    "            return cls.MALE\n",
    "        elif value == \"female\":\n",
    "            return cls.FEMALE\n",
    "        else:\n",
    "            raise ValueError(f\"Gender value should be either 'male' or 'female', {value}\")\n",
    "\n",
    "class AgeLabels(int, Enum):\n",
    "    YOUNG = 0\n",
    "    MIDDLE = 1\n",
    "    OLD = 2\n",
    "\n",
    "    @classmethod\n",
    "    def from_number(cls, value: str) -> int:\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Age value should be numeric, {value}\")\n",
    "\n",
    "        if value < 30:\n",
    "            return cls.YOUNG\n",
    "        elif value < 60:\n",
    "            return cls.MIDDLE\n",
    "        else:\n",
    "            return cls.OLD\n",
    "        \n",
    "def encode_multi_class(mask_label, gender_label, age_label) -> int:\n",
    "    return mask_label * 6 + gender_label * 3 + age_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7ebd0-5d4e-439f-a2a4-8d505fbc5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_feat_dir = '../input/data/train'\n",
    "test_feat_dir = '../input/data/eval'\n",
    "train_img_dir = '../input/data/train/images/'\n",
    "test_img_dir = '../input/data/eval/images/'\n",
    "\n",
    "'''\n",
    "MASK = Wear : 0, Incorrect : 1, Not Wear : 2\n",
    "GENDER = Male : 0, Female : 1\n",
    "AGE = ~ 29 : 0, 30 ~ 59 : 1, 60 ~ : 2\n",
    "'''\n",
    "\n",
    "# gender labeling outlier handling\n",
    "female_to_male = ['006359', '006360', '006361', '006362', '006363', '006364'] \n",
    "male_to_female = ['001498-1', '004432']\n",
    "\n",
    "num_classes = 3 * 2 * 3\n",
    "\n",
    "_file_names = {\n",
    "    \"mask1\": MaskLabels.MASK,\n",
    "    \"mask2\": MaskLabels.MASK,\n",
    "    \"mask3\": MaskLabels.MASK,\n",
    "    \"mask4\": MaskLabels.MASK,\n",
    "    \"mask5\": MaskLabels.MASK,\n",
    "    \"incorrect_mask\": MaskLabels.INCORRECT,\n",
    "    \"normal\": MaskLabels.NORMAL\n",
    "}\n",
    "\n",
    "img_paths = []\n",
    "mask_labels = []\n",
    "gender_labels = []\n",
    "age_labels = []\n",
    "multi_labels = []\n",
    "\n",
    "\n",
    "profiles = os.listdir(train_img_dir)\n",
    "for profile in profiles:\n",
    "    if profile.startswith(\".\"):  # \".\" 로 시작하는 파일은 무시합니다\n",
    "        continue\n",
    "\n",
    "    img_folder = os.path.join(train_img_dir, profile)\n",
    "    for file_name in os.listdir(img_folder):\n",
    "        _file_name, ext = os.path.splitext(file_name)\n",
    "        if _file_name not in _file_names:  # \".\" 로 시작하는 파일 및 invalid 한 파일들은 무시합니다\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(train_img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "        mask_label = _file_names[_file_name]\n",
    "\n",
    "        id, gender, race, age = profile.split(\"_\")\n",
    "        ### gender labeling outlier handling\n",
    "        if id in female_to_male:\n",
    "            gender = 'male'\n",
    "        if id in male_to_female:\n",
    "            gender = 'female'\n",
    "            \n",
    "        gender_label = GenderLabels.from_str(gender)\n",
    "        age_label = AgeLabels.from_number(age)\n",
    "\n",
    "        img_paths.append(img_path)\n",
    "        mask_labels.append(mask_label.value)\n",
    "        gender_labels.append(gender_label.value)\n",
    "        age_labels.append(age_label.value)\n",
    "        multi_labels.append(encode_multi_class(mask_label, gender_label, age_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66e143-b037-442e-9602-c8cd5be9a845",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f54134-9945-42f2-86ad-5bc1545b377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    '''\n",
    "    MASK = Wear : 0, Incorrect : 1, Not Wear : 2\n",
    "    GENDER = Male : 0, Female : 1\n",
    "    AGE = ~ 29 : 0, 30 ~ 59 : 1, 60 ~ : 2\n",
    "    '''\n",
    "    def __init__(self, img_paths, labels, transform):\n",
    "        self.img_paths = np.array(img_paths)\n",
    "        self.labels = np.array(labels)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        X, y = Image.open(self.img_paths[idx]), self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "            \n",
    "        return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bf588b-a1cd-417c-bd27-d755284b5ed0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7f4e7-6f7f-4fab-8bd8-11e979f5f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import timm\n",
    "\n",
    "class MaskModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 3):\n",
    "        super().__init__()\n",
    "        self.net = EfficientNet.from_pretrained('efficientnet-b7', in_channels=3, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class GenderModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.net = EfficientNet.from_pretrained('efficientnet-b7', in_channels=3, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "    \n",
    "class AgeModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 3):\n",
    "        super().__init__()\n",
    "        self.net = EfficientNet.from_pretrained('efficientnet-b7', in_channels=3, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91756ed3-531b-4d47-9795-2449e0c7bf43",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42ab79-507a-415e-a60e-e36c58680b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479dbf4-5382-4f65-8aef-7ce61c20cecd",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba838e7c-69ba-4bd7-9232-9dd681234c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://quokkas.tistory.com/entry/pytorch%EC%97%90%EC%84%9C-EarlyStop-%EC%9D%B4%EC%9A%A9%ED%95%98%EA%B8%B0\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): epoch loss가 개선된 후 기다리는 기간\n",
    "                            Default: 7\n",
    "            verbose (bool): True일 경우 각 epoch loss의 개선 사항 메세지 출력\n",
    "                            Default: False\n",
    "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.epoch_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "#    def __call__(self, model, optimizer, epoch_loss, epoch_acc, epoch):\n",
    "    def __call__(self, epoch_loss, epoch_acc):\n",
    "\n",
    "        score = -epoch_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            #self.save_checkpoint(model, optimizer, epoch_loss, epoch_acc, epoch)\n",
    "            self.epoch_loss_min = epoch_loss ##########\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            #self.save_checkpoint(model, optimizer, epoch_loss, epoch_acc, epoch)\n",
    "            self.epoch_loss_min = epoch_loss #########\n",
    "            self.counter = 0\n",
    "\n",
    "#     def save_checkpoint(self, model, optimizer, epoch_loss, epoch_acc, epoch):\n",
    "#         '''validation loss가 감소하면 모델을 저장한다.'''\n",
    "#         if self.verbose:\n",
    "#             print(f'Epoch loss decreased ({self.epoch_loss_min:.3f} --> {epoch_loss:.3f}).  Saving model ...')\n",
    "#         torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': epoch_loss\n",
    "#         }, f'./saved/chkpoint_model_{epoch}_{epoch_loss:.3f}_{epoch_acc:.3f}.pt')\n",
    "#         self.epoch_loss_min = epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50a06a-5db3-4f28-80da-3cd030920ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "del mask_model, gender_model, age_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db31599-160b-4cc4-a8d4-47bb6d90cd81",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add741f5-fa0b-4ed4-acf2-a9cc7545df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RGB_MEAN = [0.558, 0.512, 0.478]\n",
    "#RGB_STD = [0.218, 0.238, 0.252]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), Image.BILINEAR),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation([-19, +19]),\n",
    "    #transforms.GaussianBlur(kernel_size=501),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "####### hyper-parameter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCH = 15\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 7\n",
    "NUM_WORKERS = 8\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "\n",
    "####### train test data split\n",
    "train_mask_X, val_mask_X, train_mask_y, val_mask_y = train_test_split(img_paths, mask_labels,\n",
    "                                        test_size=TEST_SIZE, random_state=SEED)\n",
    "train_gender_X, val_gender_X, train_gender_y, val_gender_y = train_test_split(img_paths, gender_labels,\n",
    "                                        test_size=TEST_SIZE, random_state=SEED)\n",
    "train_age_X, val_age_X, train_age_y, val_age_y = train_test_split(img_paths, age_labels,\n",
    "                                        test_size=TEST_SIZE, random_state=SEED)\n",
    "_, val_X, _, val_y = train_test_split(img_paths, multi_labels,\n",
    "                                        test_size=TEST_SIZE, random_state=SEED)\n",
    "\n",
    "\n",
    "#val_indicies = set(random.choices(range(len(val_y))))\n",
    "####### dataset & dataloader\n",
    "train_mask_dataset = MyDataset(train_mask_X, train_mask_y, train_transform)\n",
    "train_mask_dataloader = DataLoader(train_mask_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "train_gender_dataset = MyDataset(train_gender_X, train_gender_y, train_transform)\n",
    "train_gender_dataloader = DataLoader(train_gender_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "train_age_dataset = MyDataset(train_age_X, train_age_y, train_transform)\n",
    "train_age_dataloader = DataLoader(train_age_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_mask_dataset = MyDataset(val_mask_X, val_mask_y, val_transform)\n",
    "val_mask_dataloader = DataLoader(val_mask_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "val_gender_dataset = MyDataset(val_gender_X, val_gender_y, val_transform)\n",
    "val_gender_dataloader = DataLoader(val_gender_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "val_age_dataset = MyDataset(val_age_X, val_age_y, val_transform)\n",
    "val_age_dataloader = DataLoader(val_age_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "val_dataset = MyDataset(val_X, val_y, val_transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "#m_losses, g_losses, a_losses = [], [], []\n",
    "\n",
    "g_losses, a_losses = [], []\n",
    "\n",
    "####### train       \n",
    "    \n",
    "mask_model = MaskModel().to(device)\n",
    "\n",
    "# for name, param in mask_model.named_parameters():\n",
    "#     if '_fc' not in name:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "m_parameters = [param for param in mask_model.parameters() if param.requires_grad]\n",
    "m_optimizer = optim.Adam(m_parameters, lr=LEARNING_RATE)\n",
    "m_criterion = nn.CrossEntropyLoss(weight=torch.tensor([1., 1., 5.]).to(device))\n",
    "mask_model.train()\n",
    "m_state = {'epoch_loss' : 999.}\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    m_epoch_loss = 0.0\n",
    "    for idx, (m_imgs, m_lbls) in enumerate(tqdm(train_mask_dataloader)):\n",
    "        m_imgs = m_imgs.to(device)\n",
    "        m_lbls = m_lbls.to(device)\n",
    "\n",
    "        m_optimizer.zero_grad()\n",
    "        \n",
    "        m_logits = mask_model(m_imgs)\n",
    "        #_, preds = torch.max(logits, 1)\n",
    "        m_loss = m_criterion(m_logits, m_lbls)\n",
    "\n",
    "        m_loss.backward()\n",
    "        m_optimizer.step()\n",
    "        \n",
    "        m_epoch_loss += m_loss.item() * m_imgs.size(0)\n",
    "    \n",
    "    m_epoch_loss = float(m_epoch_loss / len(train_mask_dataloader.dataset))\n",
    "    m_losses.append(m_epoch_loss)\n",
    "    \n",
    "    if m_epoch_loss < m_state['epoch_loss']:\n",
    "        m_state['model_state_dict'] = copy.deepcopy(mask_model.state_dict())\n",
    "        m_state['optimizer_state_dict'] = copy.deepcopy(m_optimizer.state_dict())\n",
    "        m_state['epoch_loss'] = m_epoch_loss\n",
    "\n",
    "torch.save(m_state, f\"./saved/mask_model_b7.pt\")\n",
    "\n",
    "#################\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gender_model = GenderModel().to(device)\n",
    "\n",
    "# for name, param in gender_model.named_parameters():\n",
    "#     if '_fc' not in name:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "g_parameters = [param for param in gender_model.parameters() if param.requires_grad]\n",
    "g_optimizer = optim.Adam(g_parameters, lr=LEARNING_RATE)\n",
    "g_criterion = nn.CrossEntropyLoss()\n",
    "gender_model.train()\n",
    "g_state = {'epoch_loss' : 999.}\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    g_epoch_loss = 0.0\n",
    "    for idx, (g_imgs, g_lbls) in enumerate(tqdm(train_gender_dataloader)):\n",
    "        g_imgs = g_imgs.to(device)\n",
    "        g_lbls = g_lbls.to(device)\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        g_logits = gender_model(g_imgs)\n",
    "        g_loss = g_criterion(g_logits, g_lbls)\n",
    "\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        g_epoch_loss += g_loss.item() * g_imgs.size(0)\n",
    "        \n",
    "    g_epoch_loss = float(g_epoch_loss / len(train_gender_dataloader.dataset))\n",
    "    g_losses.append(g_epoch_loss)\n",
    "    \n",
    "    if g_epoch_loss < g_state['epoch_loss']:\n",
    "        g_state['model_state_dict'] = copy.deepcopy(gender_model.state_dict())\n",
    "        g_state['optimizer_state_dict'] = copy.deepcopy(g_optimizer.state_dict())\n",
    "        g_state['epoch_loss'] = g_epoch_loss\n",
    "    \n",
    "    \n",
    "torch.save(g_state, f\"./saved/gender_model_b7.pt\")\n",
    "    \n",
    "    \n",
    "#################\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "age_model = AgeModel().to(device)\n",
    "\n",
    "# for name, param in age_model.named_parameters():\n",
    "#     if '_fc' not in name:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "a_parameters = [param for param in age_model.parameters() if param.requires_grad]\n",
    "a_optimizer = optim.Adam(a_parameters, lr=LEARNING_RATE)\n",
    "a_criterion = nn.CrossEntropyLoss(weight=torch.tensor([1., 1., 7.]).to(device))\n",
    "age_model.train()\n",
    "a_state = {'epoch_loss' : 999.}\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    a_epoch_loss = 0.0\n",
    "    for idx, (a_imgs, a_lbls) in enumerate(tqdm(train_age_dataloader)):\n",
    "        a_imgs = a_imgs.to(device)\n",
    "        a_lbls = a_lbls.to(device)\n",
    "\n",
    "        a_optimizer.zero_grad()\n",
    "        \n",
    "        a_logits = age_model(a_imgs)\n",
    "        #_, preds = torch.max(logits, 1)\n",
    "        a_loss = a_criterion(a_logits, a_lbls)\n",
    "\n",
    "        a_loss.backward()\n",
    "        a_optimizer.step()\n",
    "\n",
    "        a_epoch_loss += a_loss.item() * a_imgs.size(0)\n",
    "        \n",
    "    a_epoch_loss = float(a_epoch_loss / len(train_age_dataloader.dataset))\n",
    "    a_losses.append(a_epoch_loss)\n",
    "    \n",
    "    if a_epoch_loss < a_state['epoch_loss']:\n",
    "        a_state['model_state_dict'] = copy.deepcopy(age_model.state_dict())\n",
    "        a_state['optimizer_state_dict'] = copy.deepcopy(a_optimizer.state_dict())\n",
    "        a_state['epoch_loss'] = a_epoch_loss\n",
    "    \n",
    "\n",
    "    \n",
    "torch.save(a_state, f\"./saved/age_model_b7.pt\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54be987-390b-4b38-a572-74910e79a884",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04815da-a3ab-4a6a-a233-97dd85cdc9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### validation\n",
    "mask_preds, gender_preds, age_preds = None, None, None\n",
    "step_acc, step_f1, n_step = 0., 0., 0.\n",
    "\n",
    "mask_iter = iter(val_mask_dataloader)\n",
    "gender_iter = iter(val_gender_dataloader)\n",
    "age_iter = iter(val_age_dataloader)\n",
    "\n",
    "mask_model = MaskModel().to(device)\n",
    "m_chk_pts = torch.load('./saved/mask_model_b7.pt')\n",
    "mask_model.load_state_dict(m_chk_pts['model_state_dict'])\n",
    "gender_model = GenderModel().to(device)\n",
    "g_chk_pts = torch.load('./saved/gender_model_b7.pt')\n",
    "gender_model.load_state_dict(g_chk_pts['model_state_dict'])\n",
    "age_model = AgeModel().to(device)\n",
    "a_chk_pts = torch.load('./saved/age_model_b7.pt')\n",
    "age_model.load_state_dict(a_chk_pts['model_state_dict'])\n",
    "\n",
    "mask_model.eval()\n",
    "gender_model.eval()\n",
    "age_model.eval()\n",
    "\n",
    "for idx, (val_ims, val_lbs) in enumerate(tqdm(val_dataloader)):\n",
    "    mask_ims, mask_lbs = next(mask_iter)\n",
    "    gen_ims, gen_lbs = next(gender_iter)\n",
    "    age_ims, age_lbs = next(age_iter)\n",
    "\n",
    "    mask_logits = mask_model(mask_ims.to(device))\n",
    "    mask_logits = mask_logits.detach().cpu().numpy()\n",
    "    \n",
    "    gender_logits = gender_model(gen_ims.to(device))\n",
    "    gender_logits = gender_logits.detach().cpu().numpy()\n",
    "\n",
    "    age_logits = age_model(age_ims.to(device))\n",
    "    age_logits = age_logits.detach().cpu().numpy()\n",
    "    \n",
    "    sum_preds = []\n",
    "    for idx in range(len(mask_logits)):\n",
    "        temp = []\n",
    "        for m in mask_logits[idx]:\n",
    "            for g in gender_logits[idx]:\n",
    "                for a in age_logits[idx]:\n",
    "                    temp.append(m+g+a)\n",
    "        sum_preds.append(temp)\n",
    "    \n",
    "    sum_preds = np.argmax(sum_preds, 1)\n",
    "\n",
    "    step_acc += np.sum(np.array(val_lbs) == np.array(sum_preds))\n",
    "    step_f1 += f1_score(val_lbs, sum_preds, average='macro')\n",
    "    n_step += 1\n",
    "    \n",
    "    del mask_logits, gender_logits, age_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d8aeb-50c0-496e-9e18-bf71b031a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_f1 / n_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3692105-90e1-495f-abec-ecd6861707f9",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999784da-67d7-4a54-a873-8346ac3f7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval'\n",
    "m_chk_pts_dir = './saved/mask_model_b7.pt'\n",
    "g_chk_pts_dir = './saved/gender_model_b7.pt'\n",
    "a_chk_pts_dir = './saved/age_model_b7.pt'\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "dataset = TestDataset(image_paths, val_transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "m_chk_pts = torch.load(m_chk_pts_dir)\n",
    "mask_model = MaskModel().to(device)\n",
    "mask_model.load_state_dict(m_chk_pts['model_state_dict'])\n",
    "g_chk_pts = torch.load(g_chk_pts_dir)\n",
    "gender_model = GenderModel().to(device)\n",
    "gender_model.load_state_dict(g_chk_pts['model_state_dict'])\n",
    "a_chk_pts = torch.load(a_chk_pts_dir)\n",
    "age_model = AgeModel().to(device)\n",
    "age_model.load_state_dict(a_chk_pts['model_state_dict'])\n",
    "\n",
    "\n",
    "mask_model.eval()\n",
    "gender_model.eval()\n",
    "age_model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        \n",
    "        m_logits = mask_model(images)\n",
    "        m_logits = m_logits.detach().cpu().numpy()\n",
    "        \n",
    "        g_logits = gender_model(images)\n",
    "        g_logits = g_logits.detach().cpu().numpy()\n",
    "        \n",
    "        a_logits = age_model(images)\n",
    "        a_logits = a_logits.detach().cpu().numpy()\n",
    "        \n",
    "        add_preds = []\n",
    "        for idx in range(len(m_logits)):\n",
    "            _temp = []\n",
    "            for m in m_logits[idx]:\n",
    "                for g in g_logits[idx]:\n",
    "                    for a in a_logits[idx]:\n",
    "                        _temp.append(m+g+a)\n",
    "            add_preds.append(tempo)\n",
    "            \n",
    "        add_preds = np.argmax(add_preds, 1)\n",
    "        all_predictions.extend(add_preds)\n",
    "        \n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2aa97-14f3-4453-b709-b2f785d0c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (val_ims, val_lbs) in enumerate(tqdm(val_dataloader)):\n",
    "    mask_ims, mask_lbs = next(mask_iter)\n",
    "    gen_ims, gen_lbs = next(gender_iter)\n",
    "    age_ims, age_lbs = next(age_iter)\n",
    "    \n",
    "    mask_model.eval()\n",
    "    gender_model.eval()\n",
    "    age_model.eval()\n",
    "    \n",
    "    mask_logits = mask_model(mask_ims.to(device))\n",
    "    gender_logits = gender_model(gen_ims.to(device))\n",
    "    age_logits = age_model(age_ims.to(device))\n",
    "    \n",
    "    mask_logits = mask_logits.detach().cpu().numpy()\n",
    "    gender_logits = gender_logits.detach().cpu().numpy()\n",
    "    age_logits = age_logits.detach().cpu().numpy()\n",
    "    \n",
    "    sum_preds = []\n",
    "    for idx in range(len(mask_logits)):\n",
    "        temp = []\n",
    "        for m in mask_logits[idx]:\n",
    "            for g in gender_logits[idx]:\n",
    "                for a in age_logits[idx]:\n",
    "                    temp.append(m+g+a)\n",
    "        sum_preds.append(temp)\n",
    "    \n",
    "    sum_preds = np.argmax(sum_preds, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
