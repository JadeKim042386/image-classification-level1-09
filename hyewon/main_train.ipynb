{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, CenterCrop\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from tqdm import notebook\n",
    "import gc\n",
    "import random\n",
    "import timm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fix Seed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Train Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dir = '../input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/train.csv'\n",
    "\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "def get_transforms(need=('train', 'val'), mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = transforms.Compose([\n",
    "            CenterCrop(224),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "    if 'val' in need:\n",
    "         transformations['val'] = transforms.Compose([\n",
    "            CenterCrop(224),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "    return transformations\n",
    "\n",
    "### 마스크 여부, 성별, 나이를 mapping할 클래스를 생성합니다.\n",
    "\n",
    "class MaskLabels:\n",
    "    mask = 0\n",
    "    incorrect = 1\n",
    "    normal = 2\n",
    "\n",
    "class GenderLabels:\n",
    "    male = 0\n",
    "    female = 1\n",
    "\n",
    "class AgeGroup:\n",
    "    map_label = lambda x: 0 if int(x) < 30 else 1 if int(x) < 60 else 2\n",
    "    \n",
    "class MaskBaseDataset(Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1.jpg\": MaskLabels.mask,\n",
    "        \"mask2.jpg\": MaskLabels.mask,\n",
    "        \"mask3.jpg\": MaskLabels.mask,\n",
    "        \"mask4.jpg\": MaskLabels.mask,\n",
    "        \"mask5.jpg\": MaskLabels.mask,\n",
    "        \"incorrect_mask.jpg\": MaskLabels.incorrect,\n",
    "        \"normal.jpg\": MaskLabels.normal\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "        \n",
    "    def setup(self):\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            for file_name, label in self._file_names.items():\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                if os.path.exists(img_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_labels.append(label)\n",
    "\n",
    "                    id, gender, race, age = profile.split(\"_\")\n",
    "                    gender_label = getattr(GenderLabels, gender)\n",
    "                    age_label = AgeGroup.map_label(age)\n",
    "\n",
    "                    self.gender_labels.append(gender_label)\n",
    "                    self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "        multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image)\n",
    "            \n",
    "        return image_transform, multi_class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "transform = get_transforms()\n",
    "\n",
    "dataset = MaskBaseDataset(\n",
    "    img_dir=img_dir\n",
    ")\n",
    "\n",
    "# train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_train = len(dataset) - n_val\n",
    "train_dataset, val_dataset = data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# 각 dataset에 augmentation 함수를 설정합니다.\n",
    "train_dataset.dataset.set_transform(transform['train'])\n",
    "val_dataset.dataset.set_transform(transform['val'])\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class swinBaseModel(nn.Module):\n",
    "    def __init__(self, class_n=18):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.classify = torch.nn.Linear(in_features=1000,out_features=class_n)        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        x = self.classify(x)\n",
    "        return x\n",
    "    \n",
    "class swinTinyModel(nn.Module):\n",
    "    def __init__(self, class_n=18):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model('swin_tiny_patch4_window7_224',pretrained=True)\n",
    "        self.classify = torch.nn.Linear(in_features=1000,out_features=class_n)        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        x = self.classify(x)\n",
    "        return x\n",
    "\n",
    "class swinLargeModel(nn.Module):\n",
    "    def __init__(self, class_n=18):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model('swin_large_patch4_window7_224', pretrained=True)\n",
    "        self.classify = torch.nn.Linear(in_features=1000,out_features=class_n)        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        x = self.classify(x)\n",
    "        return x\n",
    "\n",
    "class EfficientNet7(nn.Module):\n",
    "    def __init__(self, class_n=18):\n",
    "        super().__init__()\n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b7',class_n)\n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "class EfficientNet5(nn.Module):\n",
    "    def __init__(self, class_n=18):\n",
    "        super().__init__()\n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b5',class_n)\n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class caitBaseModel(nn.Module):\n",
    "    def __init__(self, class_n=18):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model('cait_s24_224',pretrained=True)\n",
    "        self.classify = torch.nn.Linear(in_features=1000,out_features=class_n)        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        x = self.classify(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train & Valid"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(args,model, train_loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    \n",
    "    corrects, scores, running_loss  = 0., 0., 0.\n",
    "    n_iter = 0\n",
    "    for step, (images,labels) in enumerate(train_loader):\n",
    "        images = images.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if step % args.log_steps == 0:\n",
    "            print(f\"Training Steps: {step} Loss: {str(loss.item())}\")\n",
    "        \n",
    "        _,preds = torch.max(outputs, 1)\n",
    "        corrects += torch.sum(preds == labels.data)\n",
    "        running_loss += loss.item() * images.size(0) \n",
    "        scores += sklearn.metrics.f1_score(labels.data.cpu().numpy(),preds.cpu().numpy(),average='macro')\n",
    "        n_iter +=1\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    acc = corrects / len(train_loader.dataset) * 100\n",
    "    f1_scores = scores / n_iter\n",
    "    print(f'Train Loss:{train_loss} acc:{acc} F1-score:{scores}')\n",
    "    return model, train_loss, acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def validate(args, model, valid_loader,criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    corrects, scores, running_loss  = 0., 0., 0.\n",
    "    n_iter = 0\n",
    "    for step, (images,labels) in enumerate(valid_loader):\n",
    "        images = images.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        _,preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        corrects += torch.sum(preds == labels.data)\n",
    "        running_loss += loss.item() * images.size(0) \n",
    "        scores += sklearn.metrics.f1_score(labels.data.cpu().numpy(),preds.cpu().numpy(),average='macro')\n",
    "        n_iter +=1\n",
    "        \n",
    "    valid_loss = running_loss / len(valid_loader.dataset)\n",
    "    acc = corrects / len(valid_loader.dataset) * 100\n",
    "    f1_scores = scores / n_iter\n",
    "    print(f'Valid Loss:{valid_loss} Valid Acc:{acc} F1-score:{f1_scores}')\n",
    "    return valid_loss, acc, f1_scores, all_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run(args, train_data, valid_data):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if 'efficient5' in args.model_name:\n",
    "        model = EfficientNet5().to(args.device)\n",
    "    elif 'efficient' in args.model_name:\n",
    "        model = EfficientNet7().to(args.device)\n",
    "    elif 'swin_base' in args.model_name:\n",
    "        model = swinBaseModel().to(args.device)\n",
    "    elif 'swin_tiny' in args.model_name:\n",
    "        model = swinTinyModel().to(args.device)\n",
    "    elif 'swin_large' in args.model_name:\n",
    "        model = swinLargeModel().to(args.device)\n",
    "    elif 'cait' in args.model_name:\n",
    "        model = caitBaseModel().to(args.device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), args.lr, weight_decay=0.0)\n",
    "    optimizer.zero_grad()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "    criterion =nn.CrossEntropyLoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = -1\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for epoch in notebook.tqdm(range(args.n_epochs)):\n",
    "        print(f'Epoch {epoch + 1}/{args.n_epochs}')\n",
    "        train_model, train_loss, train_acc = train(args,model,train_data,optimizer,scheduler)\n",
    "        # model = train_model\n",
    "        valid_loss, valid_acc, epoch_f1, outputs = validate(args, model, valid_data)\n",
    "        print(f'train acc: {train_acc}\\tand valid acc:{valid_acc}\\n')\n",
    "        if valid_acc>best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_idx = epoch\n",
    "            best_f1 = epoch_f1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print('Best valid Acc: %d - %.1f' %(best_idx, best_acc))\n",
    "    print('Best valid f1: %d - %.1f' %(best_idx, best_f1))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    save_name =  f\"{args.model_name}_ep{args.n_epochs}_batch{args.batch_size}_lr{args.lr}_{best_acc}.pt\"\n",
    "    torch.save(model, save_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config = {}\n",
    "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config['n_epochs'] = 20\n",
    "config['batch_size'] = 16\n",
    "config['lr'] = 1e-4\n",
    "config['log_steps'] = 300\n",
    "config['model_name'] = 'swin_base'\n",
    "args = easydict.EasyDict(config)\n",
    "\n",
    "run(args,train_loader,val_loader)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}